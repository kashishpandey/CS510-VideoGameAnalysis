---
title: "Video Game Analysis"
author: "Kashish Pandey"
date: "12/13/2021"
output:
  pdf_document: default
  html_document: default
---
\section*{Introduction}
Video games have served as an engaging means of entertainment since the 1960s. With classic games such as Pong, Space Invaders, Galaxy Game, and Super Mario Bros, there has been a significant evolution within the quality of games over the past 60 years. The global gaming market is worth $173.70 billion as of 2020[11]; it's safe to stay there is a considerable market and industry for these games. There are so many different factors that draw people into the world of gaming, whether it be for relaxation, competition, or even gratification. Video games have proven to improve basic visual processes, enhance executive functioning, improve everyday skills (such as hand-eye coordination), and ease anxiety and depression to a certain degree[11]. These are reasons why the general population is drawn to video games, but I want to know what actually makes a high-quality video game and if it is possible to predict the global sales of games given all the features.  

\section*{Overall Objectives}
This project utilizes a video game dataset containing games from 1980 - 2016 from Kaggle.
The first portion of this project jumps into EDA (exploratory data analysis). Essentially,
I wanted to analyze the data before performing any models on it. I also wanted to fully clean up the model and plot the basic information out first to understand what was going on; I dropped missing values, rescaled the axis, fixed the scaling of variables, and mutated some columns in order to do so. 

Regarding the second portion of the project, I wanted to see what types of models could best predict global sales. The variables used for the model were Critic_Score,User_Score,Genre,Year_of_Release,Critic_Count, User_Count, Rating, publisher_top, developer_top,num_of_platform while predicting global sales. I used Linear Regression, Support Vectors Machines, Lasso, Ridge, and Random Forest models. I decided to use the metric of RMSE (root mean square error) to compare these models because it was a concise way to measure the actual values versus the predicted values(the outcome). I was able to see which models were overfitting and underfitting. Through hyperparameter tuning, I found optimal values to further improve the models!

\section*{Importing Libraries }
- The script at the beginning ensures that you have all the packages needed to run the following code! It is followed by importing the libraries once they are all downloaded.
- Optional: You can rerun this portion of code once everything is downloaded to get the message 
"0 packages had to be installed."

```{r}
my_packages <- c("tidyverse","testthat","ggplot2", "tree","caret","elasticnet",
                 "corrplot","kernlab","ranger")      
not_installed <- my_packages[!(my_packages %in% installed.packages()[ , "Package"])]    
if(length(not_installed)) install.packages(not_installed)               
print(paste(length(not_installed), "packages had to be installed."))   

library(tidyverse)
library(testthat)
# For plotting  
# library(ggplot2)
# Random Forest Model
library(tree)
library(ranger)
# Regression Model
library(caret)
# Lasso and Ridge Models
library(elasticnet)
# Correlation Plot 
library(corrplot)
# SVM Models (linear,poly,radial)
library(kernlab)
```

\section*{Exploratory Data Analysis}
- Reading in the file
- The Na.strings portion of the code is removing null and blank values from the dataset

```{r}
vg_sales <- read.csv("data/Video_Games_Sales_as_at_22_Dec_2016.csv",
                        sep=",",na.strings=c(""," ","NA","N/A"))

```

- Viewing the first 5 lines of the csv file 

```{r}
head(vg_sales)

```

- Checking total number of null values within the dataset

```{r}
colSums(is.na(vg_sales))

```

\section*{Dropping NULL and NA values from the datasets}
- There seem to be many missing values within this dataset
- This is because it is this dataset is the combination of 2 different datasets, and many of the original observations do not match the data from the second dataset
- Here, I am dropping all the missing values 

```{r}
vg_sales <- vg_sales[complete.cases(vg_sales), ]
colSums(is.na(vg_sales))

```

- Analyzing the internal structure of each feature
- I noticed that user_score and critic_score are different structures, but we will fix that after examining the dataset for outliers 

```{r}
str(vg_sales)

```

- Examining outlier data for sales 

```{r}
summary(vg_sales$NA_Sales)
summary(vg_sales$EU_Sales)
summary(vg_sales$JP_Sales)
summary(vg_sales$Other_Sales)
summary(vg_sales$Global_Sales)

```

- Examining outlier data for score/count

```{r}
summary(vg_sales$Critic_Score)
summary(vg_sales$Critic_Count)
summary(vg_sales$User_Count)
summary(vg_sales$User_Score)

```

- Upon analysis, critic_score seems to be an int, and user_score is num. These are two different structures, and if we want to compare the two, we need to make them the same
- Here, I am changing the user_score to int to keep it consistent with critic_score

```{r}
vg_sales$User_Score <- as.integer(vg_sales$User_Score)
summary(vg_sales$User_Score)

```

- It seems that critic_score and user_score are also out of different scales
- We need to put critic_score and user_score on the same scale
- user_score is only out of 10, and critic_score is out of 100
- By multiplying user_score by 10, both critic_score and critic_score 
are out of 100 now

```{r}
vg_sales$User_Score <- vg_sales$User_Score * 10

```

- Here, we need to alter the rating variable because there are only a few occurrences of the ratings "AO","K-A", and "RP" once within the dataset. 
- "AO" refers to Adult Only games, so we can place that into the Mature Rating
- "K-A" refers to Kids to Adults, so we can place that into the Everyone Rating
- "RP" refers to Rating Pending, so we can place that into the Everyone Rating
- I mutated the column by adding AO, K-A, and RP into their own respective categories (either the Mature rating and Everyone rating)

```{r}
vg_sales %>% count(Rating)

vg_sales <- vg_sales %>% mutate(Rating = ifelse(Rating == "AO", "M", Rating))
vg_sales <- vg_sales %>% mutate(Rating = ifelse(Rating == "K-A", "E", Rating))
vg_sales <- vg_sales %>% mutate(Rating = ifelse(Rating == "RP", "E", Rating))
vg_sales %>% count(Rating)
```

\section*{Data Visualization}

- Plotting game rating and global sales
- Teen games have the highest global sales

```{r}
rating_games_bar <- ggplot(vg_sales, aes(x = Rating,fill = Rating)) + geom_bar() + 
      theme(text = element_text(size=10)) + xlab("Rating") + ylab("Global sales")+
  theme_minimal() + ggtitle("Game Rating and Global Sales") 
 
rating_games_bar
```

- Plotting Platform and global sales
- The biggest global sales came from the platforms: Playstation 2 and Xbox360 followed by Playstation 3

```{r}
vg_sales %>% group_by(Platform) %>% 
  summarise(vg_sales = sum(Global_Sales)) %>% ggplot() + 
  geom_bar(aes(reorder(Platform, vg_sales), vg_sales), stat = "identity", 
           fill = "#645188") + 
  xlab("Platform") + ylab("Global sales") + 
  coord_flip() + theme_minimal() + ggtitle("Game Platform and Global Sales") 
```

- Plotting genre and global sales
- The top genres are Action, Sport, and Shooter

```{r}
vg_sales %>% group_by(Genre) %>% 
  summarise(vg_sales = sum(Global_Sales)) %>% ggplot() + 
  geom_bar(aes(reorder(Genre, vg_sales), vg_sales), stat = "identity", 
           fill = "#317256") + 
  xlab("Genre") + ylab("Global sales") + 
  coord_flip() + theme_minimal() +
  ggtitle("Game Genre and Global Sales") 

```

- Plotting release year and global sales by North America, Europe, Japan, and Other
- Overall, North America had the highest sales from 1980-2016

```{r}
vg_sales %>% gather(area, vg_sales, NA_Sales:Other_Sales, 
                    factor_key = TRUE) %>% 
  group_by(area,Year_of_Release) %>% 
  summarise(vg_sales = sum(vg_sales)) %>% ggplot() + 
  xlab("Year of Release") + ylab("Sales") + 
  geom_line(aes(Year_of_Release, vg_sales, group = area, color = area)) + 
  theme_minimal() + theme(legend.text = element_text(size = 7), 
                          legend.position = "bottom",
                          axis.text.x = element_text(angle = 90))+
  theme_minimal() + ggtitle("Release Year and Global Sales by Region") 
```

- Plotting the top 10 best selling games globally 
- Wii sports is the #1 game sold globally

```{r}
vg_sales %>% select(Name,Global_Sales) %>% arrange(desc(Global_Sales))%>% head(10)%>%
  ggplot(aes(x=Name,y=Global_Sales,fill= Name))+geom_bar(stat="identity")+ 
  labs(x="Game Title",y="Global Sales",
       title="Top 10 Best Selling Games")+
  theme(text = element_text(size=7),legend.position="right",
        axis.text.x=element_text(angle = 90,vjust = 0.5, hjust = 1,size=7))+
  scale_fill_brewer(name= "Game Titles", palette="Paired")

```

- Bar plot of global sales
- Overall, the plot is extremely skewed
- To combat this, we need to change x-axis to log axis because the distribution needs 
to be fixed 

```{r}
ggplot(vg_sales) + geom_histogram(aes(Global_Sales), fill = "#063970")+ ggtitle("Global Sales")+ theme_minimal()
```

- By scaling the x-axis to log axis, it fixed the axis and provided a much better
distribution (looks similar to a Gaussian distribution)

```{r}
ggplot(vg_sales) + geom_histogram(aes(Global_Sales), fill = "#063970") + 
  scale_x_log10() + ggtitle("Global Sales (with scaled axis)")+ theme_minimal()
```

- Barplot of number of titles released each year
- There seems to be a peak within the data from 2005-2009, which just means there were a substantial amount of video game titles released between those years 

```{r}
vg_sales %>% group_by(Year_of_Release) %>% 
  count() %>% ggplot() + 
  geom_bar(aes(Year_of_Release, n), stat = "identity", 
           fill = "#063970") + theme(axis.text.x = element_text(angle = 90))+
  ggtitle("Total Number of Titles Released Each Year")+
  theme_minimal()

```

- Line graph of sales each year and the total number of releases 
- There is more revenue when more titles are released

```{r}
color <- c("Titles released" = "maroon4", "Global sales" = "royalblue")
vg_sales %>% group_by(Year_of_Release) %>% 
  summarise(vg_sales = sum(Global_Sales), count = n()) %>% 
  ggplot() + xlab("Year of Release") + ylab("Titles released") +
  geom_line(aes(Year_of_Release, count, group = 1, color = "Titles released")) + 
  geom_line(aes(Year_of_Release, vg_sales, group = 1, color = "Global sales")) + 
  theme(axis.text.x = element_text(angle = 90), legend.position = "bottom") +
  scale_color_manual(name="",values = color) + theme_minimal() + ggtitle("Sales Each Year and Total Number of Titles Released")
```

- To simplify the following graphs and to make the models easier to run later I 
- am combining platforms by their respective company

- Wii, DS, 3DS, WiiU, GC, GBA are part of Nintendo
- X360, XB, XOne are part of Xbox 
- PS4, PS3, PS2, PS, PSP, PSV are part of Playstation (abbreviated to PS)
- PC is part of PC
- DC is part of Sega

```{r}
vg_sales <- vg_sales %>% mutate(platform2 = case_when(
  Platform %in% c("Wii", "DS", "3DS", "WiiU", "GC", "GBA") ~ "Nintendo",
  Platform %in% c("X360", "XB", "XOne") ~ "Xbox",
  Platform %in% c("PS4", "PS3", "PS2", "PS", "PSP", "PSV") ~ "PS",
  Platform == "PC" ~ "PC",
  Platform == "DC" ~ "Sega"
))

```

- Line graph of global sales each year for each platform
- Nintendo and Playstation both peaked near one another 

```{r}
vg_sales %>% group_by(platform2, Year_of_Release) %>%
  summarise(vg_sales = sum(Global_Sales)) %>% 
  ggplot() + xlab("Year of release") + ylab("Global Sales") + 
  geom_line(aes(Year_of_Release, vg_sales, group = platform2, color = platform2)) +
  theme(legend.text = element_text(size = 7),
        axis.text.x = element_text(angle = 90, hjust = 1, 
                                   vjust = 0.5, size = 6))+
  theme_minimal() + labs(color='Platforms') + ggtitle("Global Sales Each Year Per Platform")
```

- Bar plot of sales for each developer and global sales 
- Nintendo has the highest global sales across the board 

```{r}
vg_sales %>% group_by(Developer) %>% 
  summarise(vg_sales = sum(Global_Sales)) %>% 
  arrange(desc(vg_sales)) %>% slice(1:10) %>% 
  ggplot() + xlab("Developer") + ylab("Global Sales")+
  geom_bar(aes(reorder(Developer, vg_sales), vg_sales), 
           stat = "identity", fill = "#0095B6") +
  theme(axis.text.x = element_text(angle = 90)) +
  ggtitle("Global Sales for Each Developer")
```

- Bar plot of sales for each gaming genre and global sales

```{r}
vg_sales %>% group_by(Genre) %>%
  summarise(vg_sales = sum(Global_Sales)) %>%  
  ggplot() + 
  geom_bar(aes(reorder(Genre, vg_sales), vg_sales), stat = "identity", 
           fill = "#BC4B4B") + 
  ylab("Global Sales") + xlab("Genre") + 
  theme(axis.text.x = element_text(angle = 90, 
                                   hjust = 1, vjust = 0.5)) +
  ggtitle("Genre and Global Sales")

```

- Correlation matrix of global sales for each platform and genre
- The top two global sales come from Xbox 360 Shooter games and Playstation 3
Action games 

```{r}
vg_sales %>% group_by(Platform, Genre) %>% 
  summarise(vg_sales = sum(Global_Sales)) %>% 
  ggplot() + geom_raster(aes(Genre, Platform, fill = vg_sales)) + 
  ylab("") + xlab("") + 
  scale_fill_gradient(low = "#e4dee5", high = "blue") + 
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 0.5, hjust = 1),
        axis.text.y = element_text(size = 5),
        legend.text = element_text(size = 7)) + labs(fill = "Global Sales")+
  ggtitle("Global Sales For Each Platform and Genre")
```

\section*{Data Modeling}

- Overall, the sales vary depending on the platform, release year, and developer 
- The top developers had the highest sales 


- Publishers is categorical variable, but it contains has many values
- To combat this, we are selecting for only the top publishers 

```{r}
publishers_top <- (vg_sales %>% group_by(Publisher) %>%
                     summarise(vg_sales = sum(Global_Sales)) %>% arrange(desc(vg_sales)) %>% 
                     top_n(10) %>% distinct(Publisher))$Publisher

```

- Developers is a categorical variable, but it contains has many values
- To combat this, we are selecting for only the top developers  

```{r}
developers_top <- (vg_sales %>% group_by(Developer) %>%
                     summarise(vg_sales = sum(Global_Sales)) %>% arrange(desc(vg_sales)) %>% 
                     top_n(10) %>% distinct(Developer))$Developer

```

- Creating a new variable for whether a game is created by a top developer/publisher
- Making it binary(0,1)/(true,false)

```{r}
vg_sales <- vg_sales %>% 
  mutate(publisher_top = ifelse(Publisher %in% publishers_top, TRUE, FALSE),
         developer_top = ifelse(Developer %in% developers_top, TRUE, FALSE))
```

- Checking whether games are exclusively launched on a specific platform

```{r}
vg_sales <- vg_sales %>% group_by(Name) %>% mutate(num_of_platforms = n()) %>% ungroup(Name)

```

- Setting seed

```{r}
set.seed(2000)
```

- Training and testing data sets
- Here, I am setting the percentage of data that goes to training as 80% training 
and this would keep 20% for testing 
- I tried 90-10, 80-20, and 70-30 for the splitting of train/test, but 80% seemed to improve RMSE the best

```{r}
test_index <- createDataPartition(vg_sales$Global_Sales, p = 0.8, list = FALSE)
train_set <- vg_sales[-test_index, ]
test_set <- vg_sales[test_index, ]
```

- Including categorical data within the data 

```{r}
totalData <- rbind(train_set, test_set)
for (f in 1:length(names(totalData))) {
  levels(train_set[, f]) <- levels(totalData[, f])
}

```

\section*{Creating RMSE function}
- RMSE refers to the Root Mean Square Error
- The Root Mean Square Error is the standard deviation of our predicted errors [19]
- The equation for RMSE is as follows: $\sqrt {\frac{1}{N} \sum_{i=1}^{N} (\hat{y_{i}} - y_{i})^2}$ [17]
- Within the equation, it is taking the square root of the mean of true values subtracted by the predicted values and squaring it 

```{r}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}

```

\section*{Different Models Used}
The models I will be using are Linear Regression, Support Vectors Machines, Lasso, Ridge, and Random Forest. I had specifically picked these models out for a couple of reasons. First, it is essential to understand why I chose supervised machine learning over unsupervised machine learning techniques for this project. The critical difference between supervised and unsupervised machine learning stems from the input. Within supervised machine learning (what I used within this project), I am using known and labeled data as input. In addition, supervised machine learning is essentially learning from the training dataset by making predictions and adjusting for the correct answer. Whereas unsupervised machine learning uses unlabeled data as input and discovers the inherent structure of unlabeled data, this was not needed for this specific project.

\section*{Linear Regression Model}
- Linear regression is a technique that predicts a single output value based on training data [12]. 
- The equation for linear regression is as follows: $y = \beta_0 + \beta_1x + \varepsilon$ [18].
- $\beta_0$ refers to the intercept. 
- $\beta_1x$ refers to the slope. 
- $\varepsilon$ refers to the random error component.  
- This linear regression model is my baseline model to generally understand the model's results.
- Baseline models are an important way to interpret the model with less complexity [12].


```{r}
model_lm <- train(log(Global_Sales) ~ Critic_Score +
                     User_Score + Genre +
                     Year_of_Release + Critic_Count +
                     User_Count + Rating +
                     publisher_top + developer_top +
                     num_of_platforms, method = "lm", data = train_set)

# predicted values and RMSE
test_set$predicted_lm <- predict(model_lm, test_set)
rmse_results <- data.frame(Method = "Linear Regression",
                           RMSE = RMSE(log(test_set$Global_Sales), test_set$predicted_lm))

```

- Summary of linear regression model:
- When analyzing this model, the key value that we want to understand is the R-squared value
- R-squared is a goodness-of-fit measure; it essentially represents how well a model fits the data. The R-squared value ranges from 0 to 1. The closer R-squared is to 1, the better the fit. 
- R^2:  0.3275 
- In this case, this is not the best R-squared value, and it is showing that without using any regularization techniques, it does not have a good fit.

```{r}
summary(model_lm)

```

- Actual vs Predicted plot
- This plot showcases what the model predicted versus the actual values. It can be seen that the model had a bit of difficulty predicting accurately.

```{r}
ggplot(test_set) +
  geom_point(aes(log(Global_Sales), predicted_lm)) +
  geom_line(aes(log(Global_Sales), log(Global_Sales))) +
  xlab("Actual values") + ylab("Predicted values") + 
  ggtitle("Actual values vs Predicted values")
```

- Residual plot (Error vs Predicted)
- Residual refers to the error in a result
- Errors are the most significant for larger values of global sales; this means that heteroskedasticity present
- Heteroskedasticity refers to situations where the variance (spread of the data) of the residuals is unequal over a range of measured values [13]

```{r}
ggplot(test_set) + geom_point(aes(log(Global_Sales) - predicted_lm, Global_Sales)) +
  xlab("Error") + ylab("Global sales")
```

\section*{Support Vector Machine Linear Model}
- A Support Vector Machine finds a hyperplane (line) in N-dimensional space(N — the number of features) that distinctly classifies the data points [14]
- This model is utilizing a linear decision boundary to classify the data points 

```{r}
model_svm_linear <- train(log(Global_Sales) ~ Critic_Score + 
                     User_Score + Genre + 
                     Year_of_Release +  Critic_Count +
                     User_Count + Rating + 
                     publisher_top + developer_top + 
                     num_of_platforms, method = "svmLinear",
                   data = train_set)

# predicted value and RMSE 
test_set$predicted_svm_linear <- predict(model_svm_linear, test_set)
rmse_results <- rmse_results %>% 
  add_row(Method = "SVM Linear", 
          RMSE = RMSE(log(test_set$Global_Sales), 
                      test_set$predicted_svm_linear))

```

- Summary of SVM linear model

```{r}
summary(model_svm_linear)
```

\section*{Support Vector Machine Polynomial Model}
- This model is utilizing a polynomial decision boundary to classify the data points.
- Note: This will take several minutes to run because it is
more mathematically complex (polynomial function) than the previous models[14]. 

```{r}
model_svm_poly <- train(log(Global_Sales) ~ Critic_Score + 
                     User_Score + Genre + 
                     Year_of_Release + Critic_Count +
                     User_Count + Rating + 
                     publisher_top + developer_top + 
                     num_of_platforms, method = "svmPoly",
                   data = train_set)

# predicted value and RMSE 
test_set$predicted_svm_poly <- predict(model_svm_poly, test_set)
rmse_results <- rmse_results %>% 
  add_row(Method = "SVM Polynomial", 
          RMSE = RMSE(log(test_set$Global_Sales), 
                      test_set$predicted_svm_poly))

```

- SVM Poly Model Summary

```{r}
summary(model_svm_poly)
```

\section*{Support Vector Machine Radial Model}
- This model is utilizing Radial decision boundary to classify the data points.

```{r}
model_svm_rad <- train(log(Global_Sales) ~ Critic_Score + 
                          User_Score + Genre + 
                          Year_of_Release +  Critic_Count +
                          User_Count + Rating + 
                          publisher_top + developer_top + 
                          num_of_platforms, method = "svmRadial",
                        data = train_set)

# predicted value and RMSE 
test_set$predicted_svm_rad<- predict(model_svm_rad, test_set)
rmse_results <- rmse_results %>% 
  add_row(Method = "SVM Radial", 
          RMSE = RMSE(log(test_set$Global_Sales), 
                      test_set$predicted_svm_rad))
```

- Support Vector Machine Radial Model Summary

```{r}
summary(model_svm_rad)
```

\section*{L1 - Lasso Model}

- Lasso regression is essentially regularized linear regression.
- Regularization is the process of adding information in order to solve overfitting.
- Compared to ridge regression(L2), instead of penalizing high values, the lasso model sets these values equal to zero instead.
- There is a chance to end up with fewer features because of the method lasso uses (it is essentially keeping the more important features), and this is where lasso can have an upper hand over ridge.

```{r}
model_l1 <- train(log(Global_Sales) ~ Critic_Score +
                    User_Score + Genre +
                    Year_of_Release +  Critic_Count +
                    User_Count + Rating +
                    publisher_top + developer_top +
                    num_of_platforms, method = "lasso", data = train_set)

# predicted values and RMSE
test_set$predicted_l1 <- predict(model_l1, test_set)
rmse_results <- rmse_results %>% add_row(Method = "L1 Lasso",
                                         RMSE = RMSE(log(test_set$Global_Sales), 
                                                     test_set$predicted_l1))

```

- Summary of Lasso Model

```{r}
summary(model_l1)
```

- Actual vs Predicted graph

```{r}
ggplot(test_set) +
  geom_point(aes(log(Global_Sales), predicted_l1)) +
  geom_line(aes(log(Global_Sales), log(Global_Sales))) +
  xlab("Actual values") + ylab("Predicted values")+
  ggtitle("Actual Values vs Predicted Values")
```

- Error vs Sales

```{r}
ggplot(test_set) + geom_point(aes(log(Global_Sales) - predicted_l1, Global_Sales)) +
  xlab("Error") + ylab("Global sales")
```

\section*{L2 - Ridge Model}
- Ridge regression is essentially regularized linear regression.
- Instead of getting rid of features that do not contribute to the model, ridge regression minimizes its impact on the trained model.
- Ridge keeps all the features but is only significantly impacted by the most important features.

```{r}
model_l2 <- train(log(Global_Sales) ~ Critic_Score +
                    User_Score + Genre +
                    Year_of_Release +  Critic_Count +
                    User_Count + Rating +
                    publisher_top + developer_top +
                    num_of_platforms, method = "ridge", data = train_set)

# predicted values and RMSE
test_set$predicted_l2 <- predict(model_l2, test_set)
rmse_results <- rmse_results %>% add_row (Method = "L2 Ridge",
                           RMSE = RMSE(log(test_set$Global_Sales), test_set$predicted_l2))
```

- L2 Model Summary

```{r}
summary(model_l2)
```

- Errors vs Predicted Plot

```{r}
ggplot(test_set) +
  geom_point(aes(log(Global_Sales), predicted_l2)) +
  geom_line(aes(log(Global_Sales), log(Global_Sales))) +
  xlab("Actual values") + ylab("Predicted values")+
  ggtitle("Actual Values vs Predicted Values")
```

- Error vs Sales Plot

```{r}
ggplot(test_set) + geom_point(aes(log(Global_Sales) - predicted_l2, Global_Sales)) +
  xlab("Error") + ylab("Global sales")
```

\section*{Random Forest Model}
- Note: This model will take a few minutes to run because there are multiple decision trees
which can make the algorithm slow.
- The Random Forest Model builds decision trees on different samples and takes their majority vote for classification and average in case of regression [15].
- I am using trainControl() because it helps specify a particular number of parameters[15].
- Within trainControl(), the method = repeatedcv is being used because the parameters will repeat accordingly[15].
- splitrule = extratrees,variance because extratrees helps us specify, and variance is used because it is the default typically[15]. 
- Using method = ranger because it is performing recursive partitioning(fast implementation of random forests)[15].

```{r}
cntrl <- trainControl(method = "repeatedcv", number = 10,
                      repeats = 3)
tunegrid <- expand.grid(.mtry=c(1:5),
                        .min.node.size = seq(1, 5, 1),
                        .splitrule = c("extratrees", "variance"))
model_rf <- train(log(Global_Sales) ~ Critic_Score +
                    User_Score + Genre +
                    Year_of_Release + Critic_Count +
                    User_Count + Rating +
                    publisher_top + developer_top +
                    num_of_platforms, data = train_set,
                  method = "ranger", trControl = cntrl,
                  tuneGrid = tunegrid)

# predicted and RMSE
test_set$predicted_rf <- predict(model_rf, test_set)
rmse_results <- rmse_results %>% add_row(Method = "Random Forest",
                RMSE = RMSE(log(test_set$Global_Sales), test_set$predicted_rf))
```

- Actual vs Predicted Plot

```{r}
ggplot(test_set) +
  geom_point(aes(log(Global_Sales), predicted_rf)) +
  geom_line(aes(log(Global_Sales), log(Global_Sales))) +
  xlab("Actual values") + ylab("Predicted values") +
  labs(caption =
         paste("R-squared",
               format(model_rf$finalModel$r.squared,
                      digits = 2)))+ 
  ggtitle("Actual Values vs Predicted Values")
```

- Error vs Global Sales

```{r}
ggplot(test_set) + geom_point(aes(log(Global_Sales) - predicted_rf, Global_Sales)) +
  xlab("Error") + ylab("Global Sales")
```

\section*{Test That the RMSE is less than 2}

- The reason why I am setting the limit for RMSE to 2 is that we want the RMSE value to be as low as possible; in general, the models' RMSE were occasionally going over 2, so I set that limit based on patterns that I saw.
- I did not remove the model from the table if the RMSE was greater than 2; rather, this test serves as means of just checking whether it went over the limit or not.  

```{r}
# testing linear regression RMSE 
linear_regression_test <- rmse_results[1,2]

test_that("double",{
  expect_lt(linear_regression_test,2)
})

# testing SVM linear RMSE 
SVM_Linear_test <- rmse_results[2,2]

test_that("double",{
  expect_lt(SVM_Linear_test,2)
})

# testing SVM poly RMSE 
SVM_Polynomial_test <- rmse_results[3,2]
test_that("double",{
  expect_lt(SVM_Polynomial_test,2)
})


# testing SVM radial RMSE 
SVM_Radial_test <- rmse_results[4,2]
test_that("double",{
  expect_lt(SVM_Radial_test,2)
})

# testing L1 RMSE
L1_test <- rmse_results[5,2]
test_that("double",{
  expect_lt(L1_test,2)
})

# testing L2 RMSE
L2_test <- rmse_results[6,2]
test_that("double",{
  expect_lt(L2_test,2)
})

# testing random forest RMSE
random_forest_test <- rmse_results[7,2]
test_that("double",{
  expect_lt(random_forest_test,2)
})

```

- Comparing the RMSE values of each model

```{r}
print(rmse_results)

```


- Plotting and comparing all the models RMSE
- Random forest did best!
- Note: The lower the RMSE, the better the fit

```{r}
rmse_plot <- ggplot(rmse_results, aes(x = RMSE,y = Method, fill = Method))+
  geom_bar(stat="identity")+
  xlab("RMSE") + ylab("Model Type")

theme(text = element_text(size=10), 
      legend.position="right",
      axis.text.x=element_text(angle = 90,vjust = 0.5,hjust = 1,size=8))

rmse_plot
```

\section{Analysis of Models}
The model performance was based on the RMSE value that was outputted. The random forest model did the best compared to Linear Regression, Support Vectors Machines(linear, polynomial, radial), Lasso, and Ridge. As mentioned before, the RMSE represents the Root Mean Squared Error. The RMSE measures the difference between the predicted values and the observed values[19]. RMSE gives more importance to the highest errors, making it more sensitive to outliers[19]. But through more complicated models like Random Forest, Ridge and SVM, we can combat both outliers and overfitting. The RMSE value provides an in-depth understanding distance between actual versus predicted values. Moreover, the reason I had chosen to use the RMSE over MAE (mean absolute error), MSE (mean squared error), MAPE (mean absolute percent error), Accuracy, and others was because of the key question I wanted to answer. I wanted to answer was if it was possible to predict Global Sales given Critic Score, User Score, Genre, Year_of_Release, Critic_Count, User Count, Rating, publisher top, developer top, and num of platforms. With that in mind, when predicting a numerical output such as global sales, seeing if the model is extremely close to the true value versus not gives a good idea of the performance for this specific project.

The random forest model getting the lowest RMSE means that it had the lowest difference between predicted and observed values. Overall, it makes sense as to how random forest did the best. Through of use of 'extratrees,' it is essentially creating an extremely randomized tree[16]. Each tree nod is combined with a random choice of a certain number of attributes; the best one is determined[16]. Moreover, it is building an entirely randomized tree whose structures are independent of the target variable values of the learning sample[16].

In terms of ways I could continue to improve upon this program in the future, exploring more types of random forest models could be the best way to further understand the model as it had the best performance. Perhaps analyzing another dataset from within the last five years of the worth could also be an exciting endeavor! 

\section{References}
1. Fan, Xijin Ge, Jianli Qi and Rong. Chapter 9 The Game Sales Dataset | Learn R through Examples. https://gexijin.github.io/learnR/the-game-sales-dataset.html.

2. “R - How to Change Legend Title in Ggplot - Stack Overflow.” https://stackoverflow.com/questions/14622421/how-to-change-legend-title-in-ggplot.

3. “How to Put Labels over Geom_bar for Each Bar in R with Ggplot2 - Intellipaat Community.” https://intellipaat.com/community/16343/how-to-put-labels-over-geombar-for-each-bar-in-r-with-ggplot2.

4. “Color Hex Color Codes.” https://www.color-hex.com/.

5. Datanovia. “Top R Color Palettes to Know for Great Data Visualization” https://www.datanovia.com/en/blog/top-r-color-palettes-to-know-for-great-data-visualization/.

6. “R - Editing Legend (Text) Labels in Ggplot - Stack Overflow.” https://stackoverflow.com/questions/23635662/editing-legend-text-labels-in-ggplot.

7. “Ggplot2 Reference and Examples (Part 2) - Colours.”  http://rstudio-pubs-static.s3.amazonaws.com/5312_98fc1aba2d5740dd849a5ab797cc2c8d.html.

8. “Video Games Sales Regression Techniques.” 
https://kaggle.com/yonatanrabinovich/video-games-sales-regression-techniques.

9. "Sales of Video Games (Analysis & Visualization)."
https://www.kaggle.com/tnyont/sales-of-video-games-analysis-visualization  

10. "Analysis of Videogame sales."
https://www.kaggle.com/rohitbokade94/analysis-of-videogame-sales 

11. “Exploring the Pros and Cons of Video Gaming.” 
https://online.concordia.edu/computer-science/pros-and-cons-of-video-gaming/.

12. Montgomery, Douglas C., Elizabeth A. Peck, and G. Geoffrey Vining. 
Introduction to Linear Regression Analysis. John Wiley & Sons, 2021.
https://books.google.com/books?hl=en&lr=&id=tCIgEAAAQBAJ&oi=fnd&pg=PP13&dq=linear+regression&ots=lfufWxg0Jt&sig=sXmg2mZwoECjeYwSgguIW192PQc#v=onepage&q=linear%20regression&f=false

13. Corporate Finance Institute. “Heteroskedasticity.” 
https://corporatefinanceinstitute.com/resources/knowledge/other/heteroskedasticity/.

14. Gandhi, Rohith. “Support Vector Machine — Introduction to Machine Learning Algorithms.” Medium, July 5, 2018. https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47

15. Analytics Vidhya. “Random Forest | Introduction to Random Forest Algorithm,” June 17, 2021. https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/.

16. Geurts, Pierre, Damien Ernst, and Louis Wehenkel. “Extremely Randomized Trees.” 
Machine Learning 63, no. 1 (April 2006): 3–42. 
https://doi.org/10.1007/s10994-006-6226-1.

17. Linares, Kevin. “Linear Regression Equation in LaTex Using TexMaths under LibreOffice:” 
Kevin A. Linares (blog), September 17, 2015. https://linareskevin.wordpress.com/2015/09/17/linear-regression-equation-in-latex-using-texmaths-under-libreoffice/.

18. LaTex: Embedding Maths Equations | Data Science and Machine Learning.” 
https://www.kaggle.com/getting-started/a.

19. Moody, James. “What Does RMSE Really Mean?” Medium, September 6, 2019. https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e.
